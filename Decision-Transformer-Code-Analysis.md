# Decision-Transformer代码解读

## 1.代码结构
**此处以DT代码中的gym环境下的代码为主体代码示例。**
以文件夹位置为主体进行说明
### 1.1 data：存放gym环境实验数据，通过脚本download_d4rl_datasets.py可以下载，实验数据以pkl文件形式存储

**实验数据共包含3种实验场景，缺少论文中的Reacher场景**

#### 1.1.1 Hopper
Hopper是一个二维的单腿人物，由四个主要身体部位组成 - 顶部的躯干、中间的大腿、底部的小腿和支撑整个身体的一只脚。目标是通过在连接四个身体部位的三个铰链上施加扭矩来实现向前（右）方向移动的跳跃。

#### 1.1.2 HalfCheetah
HalfCheetah 是一个二维机器人，由 9 个连杆和 8 个连接它们的关节组成（包括两个爪子）。目标是对关节施加扭矩，使猎豹尽可能快地向前奔跑（右），根据向前移动的距离分配正奖励，向后移动分配负奖励。猎豹的躯干和头部是固定的，扭矩只能施加在大腿前后（连接到躯干）、小腿（连接到大腿）和脚（连接到小腿）的另外 6 个关节上。

#### 1.1.3 Walker2D
Walker2D是一个二维的两足机器人，由四个主要身体部位组成 - 顶部的单个躯干（两条腿在躯干后分开）、躯干下方中间的两条大腿、大腿下方底部的两条腿以及连接到腿上的两只脚，整个身体都靠在腿上。目标是通过在连接六个身体部位的六个铰链上施加扭矩，使两组脚、腿和大腿协调向前（右）方向移动。

#### 1.1.4 数据形式

（待更新......）

### 1.2 decision_transformer

模型核心代码，共包括envs、evaluation、models、training 4个部分，具体内容见下文。

### 1.3 wandb

存放wandb实验日志，需要添加‘-w True’作为主代码执行的附加信息，执行wandb的在线实验记录。

### 1.4 others

共包含以下3个文件conda_env.yml/experiment.py/readme-gym.md

conda_env.yml为环境配置参考

experiment.py为实验主代码

readme-gym.md为环境配置细节和代码运行说明

## 2. 主代码（experiment.py）

### 2.1 函数discount_cumsum

返回累计折扣因子的列表

### 2.2 函数experiment

#### 2.2.1 输入
exp_prefix：字符串，表示实验总名
variant：vars转化过的parser对象的字典表示，存储声明参数

#### 2.2.2 line30-36
分别从2个输入变量中提取有用的部分，作为函数内变量。

其中比较重要的部分是model_type变量，决定模型是dt（decision transformer）还是bc（behavior cloning），在原论文中前者是作者所提出的方法，后者是作者主要对比的一个模仿学习的方法。

#### 2.2.3 line38-63
根据不同的实验环境安排，构建对应的gym环境对象，同时指定对应环境的超参数
```
        max_ep_len = 1000
        env_targets = [3600, 1800]  # evaluation conditioning targets
        scale = 1000.  # normalization for rewards/returns
```
此外还包括执行模型behavior cloning时候所采用的的环境目标设定（评估部分有详细解释）

#### 2.2.4 line65-66
定义状态空间和动作空间维度

#### 2.2.5 line68-83
读取pkl数据，获取observation、轨迹长度、奖励等信息，其中值得注意的是将奖励模式分为2种情况。

normal表示常规奖励，在每一次动作后都会给出一个奖励信号；delayed表示稀疏/延迟奖励，在轨迹末尾才给出奖励信号。

#### 2.2.6 line85-116
首先对observation进行了归一化处理，终端输出本次实验的相关数据。（86-96）

获取其他超参数（98-101）

准备百分比行为模仿的超参数。
根据原论文的阐述，该实验主要用于对比DT结构是不是仅仅是在做模仿学习。
作者将不同百分比数据量的数据进行模仿学习，用最后的结果进行对比实验（这种实验思想可以用于多数强化学习实验）
这里先将轨迹按照奖励进行排序，仅选择前X%的数据进行模仿学习，原论文中是选取了10%、25%、40%、100%。pct_traj表示采样比例（103-113）

用于重新加权采样，以便我们根据时间步长而不是轨迹进行采样。这也是原论文中的操作，并不是采样轨迹，而是根据时间步，可以理解为采样的是不同轨迹的子集，并非完整的轨迹。（115-116）

#### 2.2.7 函数get_batch（嵌套函数）
主要功能是从pkl轨迹数据中采样出作为模型输入的序列，但并非按照完整轨迹进行采样，而是按照时间步的比例进行子集采样，获取的数据并非完整轨迹。

通过将pkl数据中的内容，解析成s, a, r, d, rtg, timesteps, mask形式，作为模型的实际输入
其中s，a，r分别表示状态、动作、奖励，其中max_len默认值取的20，也就是每个状态切片包含20个时间步内容，d表示终止状态，rtg表示累计奖励，mask表示transformer结构所使用的mask

#### 2.2.8 函数eval_episodes（嵌套函数）
嵌套函数的嵌套函数fn
待更新......

#### 2.2.9 line208-266
加载DecisionTransformer模型或MLPBCModel，并将模型放在GPU上计算（208-234）
训练优化器与学习率参数设置，采用预热训练方法（236-245）

加载Trainer，注意DT模型和BC模型是加载不同的Trainer（247-266）

#### 2.2.10 line268-280
wandb相关设置（268-275）

开始按照最大迭代次数，驱动Trainer迭代（277-280）

### 2.3 超参数
使用argparse对象管理超参数，具体参数参考表格Exp

## 3. 环境部分（envs）
本部分主要存放的是reacher2D的环境代码，由于当时的gym库并没有内置这个环境，因此这个环境是作者参考别人的程序自己编写的内容，用于生成Reacher2D的环境。
与其他4个gym实验环境不同，其实验数据并没有直接存储成pkl文件的形式，而是通过实时生成的方式给到模型。
此外，该环境对Mujoco强依赖，需要加载asset文件夹中的xml文件来进行环境定义，并在Mujoco引擎下执行交互过程。

## 4. 评估部分（evaluation）

评估部分的代码包含2个函数，其中函数evaluate_episode_rtg是在DT模型下使用的，其他模型适用的是函数evaluate_episode。
这两个函数输入是完全相同的，只是rtg函数是在内部使用了mode这个变量，而另一个函数是没有的。
rtg函数主要就是在mode上做了区分，函数内对不同的mode（normal/noise/delayed）采用了不同的处理方法。

### 4.1 函数evaluate_episode

#### 4.1.1 line18-32
s/a/r三元组初始化，target_return是目标奖励，不同任务的目标奖励不同，并且这些奖励值都是经过归一化处理的。

#### 4.1.2 line34-62
主体为一个循环体，按照规定最大回合数进行循环，默认值是1000。
主体流程按照正常的RL评估执行流程操作，但值得注意的是，**这里每次迭代后输入给模型的三元组是包含全部历史信息的**，并不是只输入当前时刻内容，作者在此处特别加上了注释强调了这一点。
函数最后返回累计的奖励值和回合长度。

### 4.2 函数evaluate_episode_rtg
如果mode都选取normal的话，总体流程差距不大，但是在目标奖励变量上，函数evaluate_episode并没有更新这个值，而rtg函数是每一个回合都会更新这个值的，且其计算的是其与当前奖励的差值。
此外在输入过程中多了一个timesteps项，具体差异需要在对模型部分解读时进一步了解。

对于其他2种模式：noise模式会在初始化状态空间时加入高斯噪声，delay模式主要影响目标奖励的计算方式，delay模式是逼近目标奖励，normal模式是将奖励差距最小。

函数最后返回累计的奖励值和回合长度。

## 5. 模型部分（models）
models部分可分为4个部分：DT模型代码，BC模型代码，轨迹模型代码，轨迹GPT2模型代码
其中轨迹模型代码是基础模型，是DT/BC模型代码的主要继承父类。

### 5.1 轨迹模型（基础模型）
轨迹模型承接自nn.Module类，包含内容精简，主要定义了初始化、前向传播和动作获取4项功能。

初始化部分只定义了动作空间维度、状态空间维度、单切片的最大时间步长度。
前向传播和动作获取都只是预留了位置，返回值和输入绝大部分内容都是缺省的。

### 5.2 Decision Transformer模型
DT模型继承自轨迹模型类，使用序列化的GPT2Model进行建模。

#### 5.2.1 输入
状态空间维度、动作空间维度、输入embedding尺寸、单切片最大时间步、最大回合长度、动作激活函数

#### 5.2.2 初始化
生成GPT模型配置文件，并加载（30-38）。
此处采用的GPT模型与Huggingface版本的唯一区别就是去掉了原本的位置embedding，因为作者会在后面添加DT自己的位置编码部分。

初始化时间步和三元组为128维embedding，作为GPT模型输入，此外也按照128维初始化了层归一化部分。（40-45）

初始化三元组的预测部分，但是DT模型不同于决策大模型，并不使用状态预测和奖励预测的部分。（47-52）

#### 5.2.3 前向传播
获取batch尺寸和序列长度，用于初始化attention_mask。（56-60）

embedding化三元组和时间步，将时间步embedding作为位置编码信息，融合进三元组。（62-71）

将经过位置编码并embedding化的三元组stack为长序列形式，作者认为这种长序列形式有利于基于状态的动作自回归。（73-83）
同样需要对attention_mask也执行相同的操作。

直接将embedding作为输入喂给模型，而非像NLP一样输入词索引。（85-89）

首先获取最后一层隐含层输出，提取最后一层输出的三元组信息。（90-94）

通过一个线性层根据最后一层的状态进行动作预测。（96-101）

#### 5.2.4 动作获取
动作获取函数的主要功能是从已知的三元组状态，进行动作预测，用于模型评估阶段。
可以理解为模型的测试所使用的部分，forward可以理解为训练所使用的部分。
整体的处理流程和评估部分的流程保持一致性，最后将经过padding后的三元组+时间步切片输入给forwad部分，从而获取动作预测值。

### 5.3 Behavior Cloning模型
模型同样继承自轨迹模型，因此同样包含初始化、前向传播、动作获取3个部分，原理比较简单所以代码十分简洁。
作者只是使用了简单的MLP结构基于当前状态进行下一个时间步的动作预测。

此处也不做过多研究，后续如有需求再进行实践......

### 5.4 GPT2模型
由于是直接使用，这里不深究，后续如有需求再进行实践......

## 6. 训练部分（training）
同样，训练部分也遵循基础类和继承类的方式构建，共包含3个部分：基础训练类、seq_trainer、act_trainer。
基础训练类为父类，seq和act训练器都继承自该父类，seq训练器负责DT模型的训练，act训练器负责BC模型的训练。

### 6.1 基础训练类
基础训练类总体可分为3个部分，初始化、迭代训练、单步训练。
其中迭代训练针对的是训练整体，也就是全部训练过程，单步训练针对的是单次迭代。

#### 6.1.1 初始化
初始化输入内容，分别包括：模型、优化器、批尺寸、批获得方法、损失形式、学习率规划、评估方法、日志字典、开始时间
其中损失形式，主要采用的监督损失的方法进行执行，使用的是动作值的均方误差。

#### 6.1.2 迭代训练
负责总体训练流程和评估的规划。

常规的训练流程，初始化损失函数、日志、训练开始时间，调用model中的训练方法，按照规定的时间步开始训练。（23-35）
首先通过单步训练获取loss值，并将loss值记录。

开始评估流程，主要应用评估部分的函数进行执行，同样用logs进行记录，在终端进行输出。（39-59）
因此，模型是完整迭代1个iteration，就进行1次评估，其中1个iteration包含10000个时间步，验证阶段包含100个时间步。

#### 6.1.3 单步训练
对每一回合进行loss求取，包含反向传播过程，最后返回loss。

### 6.2 seq_trainer
继承自基础训练类，只重写了单步训练的部分，作为DT模型的训练执行函数。

由于在基础训练类中并没有加入mask相关内容，在重写部分加入。（17-19）

在计算loss函数时，重写的单步训练方法采用了torch.nn.utils.clip_grad_norm_的梯度裁剪方法，预防梯度爆炸的情况出现。（26-29）

### 6.3 act_trainer
继承自基础训练类，只重写了单步训练的部分，作为BC模型的训练执行函数。

整体流程和常规的监督学习没什么区别，计算损失的时候同样采用动作值的均方误差。